{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先下载GO官网数据 创建全部GO的有向无环图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import Counter, deque\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from numpy.random import randint\n",
    "from sklearn.metrics import (auc, confusion_matrix, precision_recall_curve,\n",
    "                             roc_curve)\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "BIOLOGICAL_PROCESS = 'GO:0008150'\n",
    "MOLECULAR_FUNCTION = 'GO:0003674'\n",
    "CELLULAR_COMPONENT = 'GO:0005575'\n",
    "FUNC_DICT = {\n",
    "    'cc': CELLULAR_COMPONENT,\n",
    "    'mf': MOLECULAR_FUNCTION,\n",
    "    'bp': BIOLOGICAL_PROCESS\n",
    "}\n",
    "\n",
    "NAMESPACES = {\n",
    "    'cc': 'cellular_component',\n",
    "    'mf': 'molecular_function',\n",
    "    'bp': 'biological_process'\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Gene Ontology based on .obo File\n",
    "\n",
    "\n",
    "# Gene Ontology based on .obo File\n",
    "class Ontology(object):\n",
    "    def __init__(self,\n",
    "                 filename='./data/go-basic.obo',\n",
    "                 with_rels=False,\n",
    "                 include_alt_ids=True):\n",
    "        super().__init__()\n",
    "        self.ont, self.format_version, self.data_version = self.load(\n",
    "            filename, with_rels, include_alt_ids)\n",
    "        self.ic = None\n",
    "\n",
    "    # ------------------------------------\n",
    "    def load(self, filename, with_rels, include_alt_ids):\n",
    "        '''\n",
    "        filename: .obo file  GO总文件\n",
    "        with_rels: 是否包含关系 计算part_of\n",
    "        include_alt_ids: 是否包含alt_ids\n",
    "        '''\n",
    "        ont = dict()\n",
    "        format_version = []  # 存储格式版本\n",
    "        data_version = []  # 存储数据版本\n",
    "        obj = None\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                # 如果是空行 跳过\n",
    "                if not line:\n",
    "                    continue\n",
    "                    # format version line\n",
    "                # 记录格式版本\n",
    "                if line.startswith('format-version:'):\n",
    "                    l = line.split(': ')\n",
    "                    format_version = l[1]\n",
    "                # data version line\n",
    "                # 记录数据版本\n",
    "                if line.startswith('data-version:'):\n",
    "                    l = line.split(': ')\n",
    "                    data_version = l[1]\n",
    "                # item lines\n",
    "                # 如果是[Term] 说明是一个新的GO term  把新的GO ID写入字典\n",
    "                if line == '[Term]':\n",
    "                    if obj is not None:\n",
    "                        ont[obj['id']] = obj\n",
    "                    obj = dict()\n",
    "                    # 为该GO建立字典，继续存储GO相关信息 主要涉及它相关的五个关系 \n",
    "                    # four types of relations to others: is a, part of, has part, or regulates\n",
    "                    obj['is_a'] = list()\n",
    "                    obj['part_of'] = list()\n",
    "                    obj['relationship'] = list()\n",
    "                    # alternative GO term id\n",
    "                    obj['alt_ids'] = list()  # 替代GO ID\n",
    "                    # is_obsolete\n",
    "                    obj['is_obsolete'] = False\n",
    "                    continue\n",
    "                # 如果是[Typedef] 说明是一个类型定义的开始\n",
    "                elif line == '[Typedef]':\n",
    "                    if obj is not None:\n",
    "                        ont[obj['id']] = obj\n",
    "                    obj = None\n",
    "                # 否则，这一行表示术语的属性\n",
    "                else:\n",
    "                    if obj is None:\n",
    "                        continue\n",
    "                    l = line.split(': ')\n",
    "                    if l[0] == 'id':\n",
    "                        obj['id'] = l[1]\n",
    "                    elif l[0] == 'alt_id':\n",
    "                        obj['alt_ids'].append(l[1])\n",
    "                    elif l[0] == 'namespace':\n",
    "                        obj['namespace'] = l[1]\n",
    "                    elif l[0] == 'is_a':\n",
    "                        obj['is_a'].append(l[1].split(' ! ')[0])\n",
    "                    elif with_rels and l[0] == 'relationship':\n",
    "                        it = l[1].split()\n",
    "                        # add all types of relationships revised\n",
    "                        if it[0] == 'part_of':\n",
    "                            obj['part_of'].append(it[1])\n",
    "                        obj['relationship'].append([it[1], it[0]])\n",
    "                    elif l[0] == 'name':\n",
    "                        obj['name'] = l[1]\n",
    "                    # is_obsolete 过时GO ID\n",
    "                    elif l[0] == 'is_obsolete' and l[1] == 'true':\n",
    "                        obj['is_obsolete'] = True\n",
    "            if obj is not None:\n",
    "                ont[obj['id']] = obj\n",
    "        # dealing with alt_ids, why\n",
    "        for term_id in list(ont.keys()):\n",
    "            # 如果包含替代GO ID  那么将替代GO ID也加入到字典中\n",
    "            if include_alt_ids:\n",
    "                for t_id in ont[term_id]['alt_ids']:\n",
    "                    ont[t_id] = ont[term_id]\n",
    "            # 如果GO ID是过时的  那么将该GO ID从字典中删除\n",
    "            if ont[term_id]['is_obsolete']:\n",
    "                del ont[term_id]\n",
    "        # is_a -> children\n",
    "        # 对于每一个GO ID  如果有children 那么将children加入到字典中\n",
    "        # 然后把这个GO的所有is_a part of 关系的GO ID加入到children中\n",
    "        for term_id, val in ont.items():\n",
    "            if 'children' not in val:\n",
    "                val['children'] = set()\n",
    "            for p_id in val['is_a'] + val['part_of']:\n",
    "                if p_id in ont:\n",
    "                    if 'children' not in ont[p_id]:\n",
    "                        ont[p_id]['children'] = set()\n",
    "                    ont[p_id]['children'].add(term_id)\n",
    "        return ont, format_version, data_version\n",
    "\n",
    "    # ------------------------------------\n",
    "    def has_term(self, term_id):\n",
    "        return term_id in self.ont\n",
    "\n",
    "    def get_term(self, term_id):\n",
    "        if self.has_term(term_id):\n",
    "            return self.ont[term_id]\n",
    "        return None\n",
    "\n",
    "    def calculate_ic(self, annots):\n",
    "        cnt = Counter()\n",
    "        for x in annots:\n",
    "            cnt.update(x)\n",
    "        self.ic = {}\n",
    "        for go_id, n in cnt.items():\n",
    "            parents = self.get_parents(go_id)\n",
    "            if len(parents) == 0:\n",
    "                min_n = n\n",
    "            else:\n",
    "                min_n = min([cnt[x] for x in parents])\n",
    "\n",
    "            self.ic[go_id] = math.log(min_n / n, 2)\n",
    "\n",
    "    def get_ic(self, go_id):\n",
    "        if self.ic is None:\n",
    "            raise Exception('Not yet calculated')\n",
    "        if go_id not in self.ic:\n",
    "            return 0.0\n",
    "        return self.ic[go_id]\n",
    "\n",
    "    # revised 'part_of'\n",
    "    # 获得GO的所有祖先  这个获取的GO关系更加广泛 获得了GO的父元素，以及父元素的所有父元素 相当于把这个种族的家族都拿遍了\n",
    "    def get_ancestors(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        q = deque()  # 双端队列\n",
    "        q.append(term_id)  # 先将当前的GO加入队列中\n",
    "        # 当队列不为空时，从队列中弹出一个GO术语，如果这个GO不在term_set中，就加入，并把这个GO的所有父元素加入队列\n",
    "        while (len(q) > 0):\n",
    "            t_id = q.popleft()\n",
    "            if t_id not in term_set:\n",
    "                term_set.add(t_id)\n",
    "                for parent_id in (self.ont[t_id]['is_a'] +\n",
    "                                  self.ont[t_id]['part_of']):\n",
    "                    if parent_id in self.ont:\n",
    "                        q.append(parent_id)\n",
    "        # terms_set.remove(term_id)\n",
    "        return term_set\n",
    "\n",
    "    # revised\n",
    "    # 获得GO的父节点  这个获取是小范围的，只获得与指定GO关系为is_a part of 的GO\n",
    "    def get_parents(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        # 获得GO ID的所有父元素： 父元素 与这个GO关系为is_a part of的GO\n",
    "        for parent_id in (self.ont[term_id]['is_a'] +\n",
    "                          self.ont[term_id]['part_of']):\n",
    "            if parent_id in self.ont:\n",
    "                term_set.add(parent_id)\n",
    "        return term_set\n",
    "\n",
    "    # get the root terms(only is_a)\n",
    "    # 获得的是GO的根祖先 即GO的is_a关系以及父节点的所有is_a关系  就是祖先集合的只包含is_a关系的版本 近亲\n",
    "    def get_root_ancestors(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        q = deque()\n",
    "        q.append(term_id)\n",
    "        while (len(q) > 0):\n",
    "            t_id = q.popleft()\n",
    "            if t_id not in term_set:\n",
    "                term_set.add(t_id)\n",
    "                for parent_id in self.ont[t_id]['is_a']:\n",
    "                    if parent_id in self.ont:\n",
    "                        q.append(parent_id)\n",
    "        # terms_set.remove(term_id)\n",
    "        return term_set\n",
    "\n",
    "    # 获得GO的所有根元素\n",
    "    def get_roots(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        root_set = set()\n",
    "        for term in self.get_root_ancestors(term_id): # 遍历该GO的所有根祖先\n",
    "            if term not in self.ont:\n",
    "                continue\n",
    "            # 如果该GO的父元素为空 那么就是根元素\n",
    "            if len(self.get_parents(term)) == 0:\n",
    "                root_set.add(term)\n",
    "\n",
    "        return root_set\n",
    "\n",
    "    def get_namespace_terms(self, namespace):\n",
    "        terms = set()\n",
    "        for go_id, obj in self.ont.items():\n",
    "            if obj['namespace'] == namespace:\n",
    "                terms.add(go_id)\n",
    "        return terms\n",
    "\n",
    "    def get_namespace(self, term_id):\n",
    "        return self.ont[term_id]['namespace']\n",
    "\n",
    "    # all children\n",
    "    # 获得该GO的所有孩子元素 BFS搜索 从term_id开始逐层向下搜索所有的子术语 直到没有  这个获得了所有的孩子 即获得GO的子元素后，又纳入子元素的子元素\n",
    "    def get_term_set(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        q = deque()\n",
    "        q.append(term_id)\n",
    "        while len(q) > 0:\n",
    "            t_id = q.popleft()\n",
    "            if t_id not in term_set:\n",
    "                term_set.add(t_id)\n",
    "                for ch_id in self.ont[t_id]['children']:\n",
    "                    q.append(ch_id)\n",
    "        return term_set\n",
    "\n",
    "    # only one layer children\n",
    "    # 获得该GO的一层孩子元素  只获得一层孩子元素\n",
    "    def get_child_set(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        if term_id not in term_set:\n",
    "            for ch_id in self.ont[term_id]['children']:\n",
    "                term_set.add(ch_id)\n",
    "        return term_set\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# functions for evaluation\n",
    "def get_matrix(labels, preds, threshold=0.3):\n",
    "    preds = preds.flatten()\n",
    "    preds[preds >= threshold] = 1\n",
    "    preds = preds.astype('int8')\n",
    "    tn, fp, fn, tp = confusion_matrix(labels.flatten(), preds).ravel()\n",
    "    return tn, fp, fn, tp\n",
    "\n",
    "\n",
    "def get_level_matrix(labels, preds, level, threshold=0.3):\n",
    "    preds = preds[..., level]\n",
    "    preds = preds.flatten()\n",
    "    preds[preds >= threshold] = 1\n",
    "    preds = preds.astype('int8')\n",
    "    labels = labels[..., level]\n",
    "    tn, fp, fn, tp = confusion_matrix(labels.flatten(), preds).ravel()\n",
    "    return tn, fp, fn, tp\n",
    "\n",
    "\n",
    "def compute_roc(labels, preds):\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr, tpr, _ = roc_curve(labels.flatten(), preds.flatten())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "def compute_aupr_level(labels, preds, level):\n",
    "    labels = labels[..., level]\n",
    "    preds = preds[..., level]\n",
    "    precision, recall, _ = precision_recall_curve(labels.flatten(),\n",
    "                                                  preds.flatten())\n",
    "    aupr = auc(recall, precision)\n",
    "    return aupr\n",
    "\n",
    "\n",
    "def compute_aupr(labels, preds):\n",
    "    precision, recall, _ = precision_recall_curve(labels.flatten(),\n",
    "                                                  preds.flatten())\n",
    "    aupr = auc(recall, precision)\n",
    "    return aupr\n",
    "\n",
    "\n",
    "# set random seed\n",
    "def set_random_seed(seed=10, deterministic=False, benchmark=False):\n",
    "    # random.seed(seed)\n",
    "    # np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    if benchmark:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "# contrast training\n",
    "class con_pair_dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 con_pair,\n",
    "                 contrast_dict,\n",
    "                 terms,\n",
    "                 terms_dict,\n",
    "                 neg_num=80,\n",
    "                 neg=0.5,\n",
    "                 neg1_len=0.25):\n",
    "        super().__init__()\n",
    "        self.len_df = len(con_pair)\n",
    "        self.n_cc = list(contrast_dict['n_cc'])\n",
    "        self.n_bp = list(contrast_dict['n_bp'])\n",
    "        self.n_mf = list(contrast_dict['n_mf'])\n",
    "        self.terms = terms\n",
    "        self.contrast_dict = contrast_dict\n",
    "        self.terms_dict = terms_dict\n",
    "        self.neg_num = neg_num\n",
    "        self.con_pair = con_pair\n",
    "        self.neg = neg\n",
    "        self.neg1_len = neg1_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        terms_list = [self.con_pair[idx][0], self.con_pair[idx][1]]\n",
    "        negs1 = set()\n",
    "        neg1_len = min(len(self.con_pair[idx][2][0]),\n",
    "                       int(self.neg_num * self.neg1_len))\n",
    "        if neg1_len > 0:\n",
    "            negs1 = set(random.sample(self.con_pair[idx][2][0], k=neg1_len))\n",
    "        negs1 = list(negs1)\n",
    "        random.shuffle(negs1)\n",
    "\n",
    "        negs2 = set()\n",
    "        neg2_len = int((self.neg_num - neg1_len) * self.neg)\n",
    "        if len(self.contrast_dict[self.con_pair[idx][0]]) <= neg2_len:\n",
    "            negs2 = negs2 | set(\n",
    "                random.sample(self.contrast_dict[self.con_pair[idx][0]],\n",
    "                              k=len(\n",
    "                                  self.contrast_dict[self.con_pair[idx][0]])))\n",
    "            negs2 = negs2 | set(\n",
    "                random.sample(self.contrast_dict[self.con_pair[idx][0]],\n",
    "                              k=neg2_len -\n",
    "                              len(self.contrast_dict[self.con_pair[idx][0]])))\n",
    "        else:\n",
    "            negs2 = negs2 | set(\n",
    "                random.sample(self.contrast_dict[self.con_pair[idx][0]],\n",
    "                              k=neg2_len))\n",
    "        negs2 = list(negs2)\n",
    "        random.shuffle(negs2)\n",
    "\n",
    "        neg_len = neg1_len + neg2_len\n",
    "        neg_num = self.neg_num - neg_len\n",
    "        negs3 = set()\n",
    "        if self.contrast_dict[self.terms[terms_list[0]]] == 'GO:0005575':\n",
    "            while len(negs3) < neg_num // 3:\n",
    "                m = randint(0, len(self.n_mf) - 1)\n",
    "                if self.terms_dict[self.n_mf[m]] not in negs3:\n",
    "                    negs3.add(self.terms_dict[self.n_mf[m]])\n",
    "            while len(negs3) < neg_num:\n",
    "                m = randint(0, len(self.n_bp) - 1)\n",
    "                if self.terms_dict[self.n_bp[m]] not in negs3:\n",
    "                    negs3.add(self.terms_dict[self.n_bp[m]])\n",
    "        elif self.contrast_dict[self.terms[terms_list[0]]] == 'GO:0003674':\n",
    "            while len(negs3) < neg_num // 5:\n",
    "                m = randint(0, len(self.n_cc) - 1)\n",
    "                if self.terms_dict[self.n_cc[m]] not in negs3:\n",
    "                    negs3.add(self.terms_dict[self.n_cc[m]])\n",
    "            while len(negs3) < neg_num:\n",
    "                m = randint(0, len(self.n_bp) - 1)\n",
    "                if self.terms_dict[self.n_bp[m]] not in negs3:\n",
    "                    negs3.add(self.terms_dict[self.n_bp[m]])\n",
    "        elif self.contrast_dict[self.terms[terms_list[0]]] == 'GO:0008150':\n",
    "            while len(negs3) < neg_num // 3:\n",
    "                m = randint(0, len(self.n_cc) - 1)\n",
    "                if self.terms_dict[self.n_cc[m]] not in negs3:\n",
    "                    negs3.add(self.terms_dict[self.n_cc[m]])\n",
    "            while len(negs3) < neg_num:\n",
    "                m = randint(0, len(self.n_mf) - 1)\n",
    "                if self.terms_dict[self.n_mf[m]] not in negs3:\n",
    "                    negs3.add(self.terms_dict[self.n_mf[m]])\n",
    "        negs3 = list(negs3)\n",
    "        random.shuffle(negs3)\n",
    "\n",
    "        neg1_num = [neg1_len for i in range(neg1_len)]\n",
    "        neg2_num = [neg2_len for i in range(neg2_len)]\n",
    "        neg3_num = [neg_num for i in range(neg_num)]\n",
    "        neg_num = neg1_num + neg2_num + neg3_num\n",
    "        neg_num = 1 / np.array(neg_num)\n",
    "        terms_list = terms_list + negs1 + negs2 + negs3\n",
    "        return torch.LongTensor(terms_list).view(\n",
    "            len(terms_list)), torch.from_numpy(neg_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用GO类 计算获得每个GO的祖先和children 保存成Pkl文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "from collections import defaultdict as ddt\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='extract all terms in a go.obo file.',\n",
    "                                 add_help=False)\n",
    "parser.add_argument('--go-file',\n",
    "                    '-gf',\n",
    "                    default='./data/go-basic.obo',\n",
    "                    type=str,\n",
    "                    help='go file downloaded from Gene Ontology website')\n",
    "parser.add_argument('--terms-file',\n",
    "                    '-tf',\n",
    "                    default='./data/terms_all.pkl',\n",
    "                    type=str,\n",
    "                    help='A DataFrame stored all terms')\n",
    "parser.add_argument('--out-ancestor',\n",
    "                    '-oa',\n",
    "                    default='./data/go_ancestor.pkl',\n",
    "                    type=str,\n",
    "                    help='output file for ancestor')\n",
    "parser.add_argument('--out-children',\n",
    "                    '-oc',\n",
    "                    default='./data/go_children.pkl',\n",
    "                    type=str,\n",
    "                    help='output file for children')\n",
    "\n",
    "\n",
    "\n",
    "def main(go_file, terms_all_file, out_ancestor, out_children):\n",
    "    # INPUT FILES\n",
    "    go = Ontology(go_file, with_rels=True, include_alt_ids=False)\n",
    "    # 读取所有GO ID\n",
    "    with open(terms_all_file, 'rb') as fd:\n",
    "        terms = pickle.load(fd)\n",
    "        terms = list(terms['terms'])\n",
    "    terms_set = set(terms)  # GO ID 集合\n",
    "    terms_dict = {v: i for i, v in enumerate(terms)}  # 构建GO映射 如{'GO:0000001': 0, 'GO:0000002': 1, ...}\n",
    "    # one layer parents, no self\n",
    "    # parents_dict 获得并存储每个GO术语的父元素—— is_a part of\n",
    "    parents_dict = ddt(set)  # 创建默认空字典 如果访问key不存在时不报错，返回默认值\n",
    "    for i in range(len(terms)):\n",
    "        parents_dict[terms[i]] = terms_set.intersection(go.get_parents(terms[i]))\n",
    "    \n",
    "    # all ancestors, no self\n",
    "    # ancestor_dict 获得每个GO的所有祖先元素 这个获取比父元素更为广泛 获得的是自己的父元素，以及所有父元素的父元素\n",
    "    ancestor_dict = ddt(set)\n",
    "    for i in range(len(terms)):\n",
    "        temp_set = go.get_ancestors(terms[i])\n",
    "        temp_set.remove(terms[i])\n",
    "        ancestor_dict[terms[i]] = terms_set.intersection(temp_set)\n",
    "    \n",
    "    # 获得所有GO各自的根元素 一个GO有相关的父元素，通过BFS找到这个GO家族的根元素 可能有多个\n",
    "    root_dict = ddt(set)\n",
    "    for i in range(len(terms)):\n",
    "        root_dict[terms[i]] = go.get_roots(terms[i])\n",
    "    # 这里简化了字典 只保留了一个根元素 如{'GO:0000001': {1,2,3,4,5}} 变为{'GO:0000001': 1}\n",
    "    for k, v in root_dict.items():\n",
    "        root_dict[k] = list(v)[0]\n",
    "    # 获得所有GO各自的所有子元素 即获得该GO的子元素后，继续获得子元素的子元素，直到没有子元素\n",
    "    child_dict = ddt(set)\n",
    "    for i in range(len(terms)):\n",
    "        child_dict[terms[i]] = terms_set.intersection(go.get_term_set(terms[i]))\n",
    "    # 获得所有GO各自的所有子元素 这个只有一层子元素 只获得该GO的子元素 不往后获得孙子辈的\n",
    "    child_one_dict = ddt(set)\n",
    "    for i in range(len(terms)):\n",
    "        child_one_dict[terms[i]] = terms_set.intersection(\n",
    "            go.get_child_set(terms[i]))\n",
    "\n",
    "    go_ancestor = ddt(list)\n",
    "    go_children = ddt(list)\n",
    "    count = 0\n",
    "    for i in terms:  # 遍历所有 GO ID\n",
    "        temp_anc_set = ancestor_dict[i]  # 获得该GO的所有祖先元素 即该GO的父元素，以及所有父元素的父元素\n",
    "        temp_child_set = go.get_term_set(i)  # 获得该GO的所有子元素 即该GO的子元素，以及所有子元素的子元素\n",
    "\n",
    "        go_ancestor[i] = list(temp_anc_set)\n",
    "        go_children[i] = list(temp_child_set)\n",
    "        print('{} is ok'.format(count))\n",
    "        count += 1\n",
    "\n",
    "\n",
    "    with open('./data/go_ancestor.pkl', 'wb') as fd:\n",
    "        pickle.dump(go_ancestor, fd)\n",
    "\n",
    "    with open('./data/go_children.pkl', 'wb') as fd:\n",
    "        pickle.dump(go_children, fd)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # args = parser.parse_args()\n",
    "    # main(args.go_file, args.terms_file, args.out_ancestor, args.out_children)\n",
    "\n",
    "    import pickle\n",
    "\n",
    "    with open('./data/go_ancestor.pkl', 'rb') as f:\n",
    "        ancestor_data = pickle.load(f)\n",
    "    print(len(ancestor_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取pkl文件 检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取pkl文件 查看\n",
    "'''\n",
    "所有GO的ancestor与children已经分别存到了go_ancestor.pkl与go_children.pkl文件中\n",
    "接下来只需要先把所有GO存到List中 遍历每个GO\n",
    "根据GO id索引到其children 将那些在list中的child id存到dict中即可\n",
    "'''\n",
    "\n",
    "import pickle\n",
    "\n",
    "# 读取pkl文件\n",
    "with open('./GO/go_children.pkl', 'rb') as f:\n",
    "    go_ancestors = pickle.load(f)\n",
    "print(go_ancestors['GO:0051052'])\n",
    "print('GO:0051973' in go_ancestors['GO:0051052'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来 读取UniProt整理的固定GO id 搭建这部分GO的稀疏网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 根据UniProt整合的GO ID列表是固定的 \n",
    "\n",
    "mf_keys = ['GO:0001618', 'GO:0003677', 'GO:0003723', 'GO:0003774', 'GO:0003824', 'GO:0003924', 'GO:0005198', 'GO:0005215', 'GO:0008092', \n",
    "           'GO:0008289', 'GO:0009975', 'GO:0016209', 'GO:0016491', 'GO:0016740', 'GO:0016787', 'GO:0016829', 'GO:0016853', 'GO:0016874', \n",
    "           'GO:0031386', 'GO:0038024', 'GO:0042393', 'GO:0044183', 'GO:0045182', 'GO:0045735', 'GO:0048018', 'GO:0060089', 'GO:0060090',\n",
    "           'GO:0090729', 'GO:0098631', 'GO:0098772', 'GO:0120274', 'GO:0140096', 'GO:0140097', 'GO:0140098', 'GO:0140104', 'GO:0140110',\n",
    "           'GO:0140223', 'GO:0140299', 'GO:0140313', 'GO:0140657', 'GO:0003674']\n",
    "\n",
    "bp_keys = ['GO:0000278', 'GO:0000910', 'GO:0002181', 'GO:0002376', 'GO:0003012', 'GO:0003013', 'GO:0003014', 'GO:0003016', 'GO:0005975',\n",
    "           'GO:0006091', 'GO:0006260', 'GO:0006281', 'GO:0006310', 'GO:0006325', 'GO:0006351', 'GO:0006355', 'GO:0006399', 'GO:0006457',\n",
    "           'GO:0006486', 'GO:0006520', 'GO:0006575', 'GO:0006629', 'GO:0006766', 'GO:0006790', 'GO:0006886', 'GO:0006913', 'GO:0006914',\n",
    "           'GO:0006954', 'GO:0007005', 'GO:0007010', 'GO:0007018', 'GO:0007031', 'GO:0007040', 'GO:0007059', 'GO:0007155', 'GO:0007163',\n",
    "           'GO:0012501', 'GO:0015979', 'GO:0016071', 'GO:0016073', 'GO:0016192', 'GO:0022414', 'GO:0022600', 'GO:0023052', 'GO:0030154',\n",
    "           'GO:0030163', 'GO:0030198', 'GO:0031047', 'GO:0032200', 'GO:0034330', 'GO:0042060', 'GO:0044782', 'GO:0048856', 'GO:0048870',\n",
    "           'GO:0050877', 'GO:0050886', 'GO:0051604', 'GO:0055085', 'GO:0055086', 'GO:0061007', 'GO:0061024', 'GO:0065003', 'GO:0071554',\n",
    "           'GO:0071941', 'GO:0072659', 'GO:0098542', 'GO:0098754', 'GO:0140013', 'GO:0140014', 'GO:0140053', 'GO:1901135', 'GO:0008150']\n",
    "\n",
    "cc_keys = ['GO:0000228', 'GO:0005576', 'GO:0005615', 'GO:0005618', 'GO:0005634', 'GO:0005635', 'GO:0005654', 'GO:0005694', 'GO:0005730',\n",
    "           'GO:0005739', 'GO:0005764', 'GO:0005768', 'GO:0005773', 'GO:0005777', 'GO:0005783', 'GO:0005794', 'GO:0005811', 'GO:0005815',\n",
    "           'GO:0005829', 'GO:0005840', 'GO:0005856', 'GO:0005886', 'GO:0005929', 'GO:0009536', 'GO:0009579', 'GO:0030312', 'GO:0031012',\n",
    "           'GO:0031410', 'GO:0043226', 'GO:0005575']\n",
    "go_ids = mf_keys + bp_keys + cc_keys\n",
    "# 去除空字符串\n",
    "if '' in go_ids:\n",
    "    go_ids.remove('')\n",
    "\n",
    "# 读取go_children.pkl文件\n",
    "with open('./data/go_children.pkl', 'rb') as file:\n",
    "    go_children = pickle.load(file)\n",
    "\n",
    "# 初始化一个空字典来存储结果\n",
    "result = {}\n",
    "\n",
    "# 遍历go_ids列表\n",
    "for go_id in go_ids:\n",
    "    # 根据GO id索引找到这个GO的所有children\n",
    "    row_children = go_children.get(go_id, [])\n",
    "    # 只保留在go_ids列表中的children\n",
    "    children = [child for child in row_children if child in go_ids and child != go_id]\n",
    "    # 将结果存入字典\n",
    "    result[go_id] = children\n",
    "\n",
    "# 将字典写入txt文件\n",
    "with open('./data/noise_free/eSOL_go/uniprot_conform_go2go.txt', 'w') as file:\n",
    "    for go_id, child in result.items():\n",
    "        if child != []:\n",
    "            file.write(go_id + ': ' + ', '.join(child) + '\\n')\n",
    "        else:\n",
    "            file.write(go_id + ':' + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备数据最后一步 将GO-GO.txt与protein-GO.txt整合 整理成SparseGO论文需要的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GO-GO: GO_uniprot_conform_go_children.txt  GO之间的child关系记录\n",
    "protein-GO: uniprot_protein2go_conform_test_mut.txt  protein与GO的关系记录 与数据集有关\n",
    "'''\n",
    "\n",
    "# 先读取GO-GO txt\n",
    "with open('./data/noise_free/eSOL_go/eSOL_test_GO-GO.txt', 'w') as new_file:\n",
    "    # 打开原始的GO-GO txt文件\n",
    "    with open('./data/noise_free/eSOL_go/uniprot_conform_go2go.txt', 'r') as old_file:\n",
    "        # 遍历原始文件的每一行\n",
    "        for line in old_file:\n",
    "            # 分割行，获取父节点和子节点列表\n",
    "            parts = line.strip().split(': ')\n",
    "            # 如果没有子节点，跳过这一行\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            parent, children = parts\n",
    "            # 分割子节点列表，获取每一个子节点\n",
    "            children = children.split(', ')\n",
    "            # 遍历每一个子节点\n",
    "            for child in children:\n",
    "                # 写入新的txt文件\n",
    "                new_file.write(f'{parent} {child} default\\n')\n",
    "\n",
    "# 再读取GO-protein txt\n",
    "with open('./data/noise_free/eSOL_go/eSOL_test_GO-protein.txt', 'w') as new_file:\n",
    "    # 打开原始的GO-GO txt文件\n",
    "    with open('./data/noise_free/eSOL_go/eSOL_test_100_uniprot_conform_protein2go.txt', 'r') as old_file:\n",
    "        # 遍历原始文件的每一行\n",
    "        for line in old_file:\n",
    "            # 分割行，获取父节点和子节点列表\n",
    "            parts = line.strip().split(': ')\n",
    "            # 如果没有子节点，跳过这一行\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            protein, gos = parts\n",
    "            # 分割子节点列表，获取每一个子节点\n",
    "            gos = gos.split(', ')\n",
    "            # 遍历每一个子节点\n",
    "            for go in gos:\n",
    "                # 写入新的txt文件\n",
    "                new_file.write(f'{go} {protein} protein\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
