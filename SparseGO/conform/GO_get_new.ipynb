{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原先GO获得是做成了硬编码 太稀疏了\n",
    "现在在原先得到蛋白质id对应的GO类别的基础上做修改，根据UniProt的GO 注释展示的各小类总括构建mutil-hot encoding\n",
    "将几千几万的维度降维到几百 并划分了BP MF CC三类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GO类——处理GO官网下载的数据 用于获得每个GO的祖先 ID 便于后续归类查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import Counter, deque\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from numpy.random import randint\n",
    "from sklearn.metrics import (auc, confusion_matrix, precision_recall_curve,\n",
    "                             roc_curve)\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "BIOLOGICAL_PROCESS = 'GO:0008150'\n",
    "MOLECULAR_FUNCTION = 'GO:0003674'\n",
    "CELLULAR_COMPONENT = 'GO:0005575'\n",
    "FUNC_DICT = {\n",
    "    'cc': CELLULAR_COMPONENT,\n",
    "    'mf': MOLECULAR_FUNCTION,\n",
    "    'bp': BIOLOGICAL_PROCESS\n",
    "}\n",
    "\n",
    "NAMESPACES = {\n",
    "    'cc': 'cellular_component',\n",
    "    'mf': 'molecular_function',\n",
    "    'bp': 'biological_process'\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Gene Ontology based on .obo File\n",
    "\n",
    "\n",
    "# Gene Ontology based on .obo File\n",
    "class Ontology(object):\n",
    "    def __init__(self,\n",
    "                 filename=r'E:\\xj\\Paper\\PO2GO\\protein-annotation-master\\data\\go-basic.obo',\n",
    "                 with_rels=False,\n",
    "                 include_alt_ids=True):\n",
    "        super().__init__()\n",
    "        self.ont, self.format_version, self.data_version = self.load(\n",
    "            filename, with_rels, include_alt_ids)\n",
    "        self.ic = None\n",
    "\n",
    "    # ------------------------------------\n",
    "    def load(self, filename, with_rels, include_alt_ids):\n",
    "        '''\n",
    "        filename: .obo file  GO总文件\n",
    "        with_rels: 是否包含关系 计算part_of\n",
    "        include_alt_ids: 是否包含alt_ids\n",
    "        '''\n",
    "        ont = dict()\n",
    "        format_version = []  # 存储格式版本\n",
    "        data_version = []  # 存储数据版本\n",
    "        obj = None\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                # 如果是空行 跳过\n",
    "                if not line:\n",
    "                    continue\n",
    "                    # format version line\n",
    "                # 记录格式版本\n",
    "                if line.startswith('format-version:'):\n",
    "                    l = line.split(': ')\n",
    "                    format_version = l[1]\n",
    "                # data version line\n",
    "                # 记录数据版本\n",
    "                if line.startswith('data-version:'):\n",
    "                    l = line.split(': ')\n",
    "                    data_version = l[1]\n",
    "                # item lines\n",
    "                # 如果是[Term] 说明是一个新的GO term  把新的GO ID写入字典\n",
    "                if line == '[Term]':\n",
    "                    if obj is not None:\n",
    "                        ont[obj['id']] = obj\n",
    "                    obj = dict()\n",
    "                    # 为该GO建立字典，继续存储GO相关信息 主要涉及它相关的五个关系 \n",
    "                    # four types of relations to others: is a, part of, has part, or regulates\n",
    "                    obj['is_a'] = list()\n",
    "                    obj['part_of'] = list()\n",
    "                    obj['relationship'] = list()\n",
    "                    # alternative GO term id\n",
    "                    obj['alt_ids'] = list()  # 替代GO ID\n",
    "                    # is_obsolete\n",
    "                    obj['is_obsolete'] = False\n",
    "                    continue\n",
    "                # 如果是[Typedef] 说明是一个类型定义的开始\n",
    "                elif line == '[Typedef]':\n",
    "                    if obj is not None:\n",
    "                        ont[obj['id']] = obj\n",
    "                    obj = None\n",
    "                # 否则，这一行表示术语的属性\n",
    "                else:\n",
    "                    if obj is None:\n",
    "                        continue\n",
    "                    l = line.split(': ')\n",
    "                    if l[0] == 'id':\n",
    "                        obj['id'] = l[1]\n",
    "                    elif l[0] == 'alt_id':\n",
    "                        obj['alt_ids'].append(l[1])\n",
    "                    elif l[0] == 'namespace':\n",
    "                        obj['namespace'] = l[1]\n",
    "                    elif l[0] == 'is_a':\n",
    "                        obj['is_a'].append(l[1].split(' ! ')[0])\n",
    "                    elif with_rels and l[0] == 'relationship':\n",
    "                        it = l[1].split()\n",
    "                        # add all types of relationships revised\n",
    "                        if it[0] == 'part_of':\n",
    "                            obj['part_of'].append(it[1])\n",
    "                        obj['relationship'].append([it[1], it[0]])\n",
    "                    elif l[0] == 'name':\n",
    "                        obj['name'] = l[1]\n",
    "                    # is_obsolete 过时GO ID\n",
    "                    elif l[0] == 'is_obsolete' and l[1] == 'true':\n",
    "                        obj['is_obsolete'] = True\n",
    "            if obj is not None:\n",
    "                ont[obj['id']] = obj\n",
    "        # dealing with alt_ids, why\n",
    "        for term_id in list(ont.keys()):\n",
    "            # 如果包含替代GO ID  那么将替代GO ID也加入到字典中\n",
    "            if include_alt_ids:\n",
    "                for t_id in ont[term_id]['alt_ids']:\n",
    "                    ont[t_id] = ont[term_id]\n",
    "            # 如果GO ID是过时的  那么将该GO ID从字典中删除\n",
    "            if ont[term_id]['is_obsolete']:\n",
    "                del ont[term_id]\n",
    "        # is_a -> children\n",
    "        # 对于每一个GO ID  如果有children 那么将children加入到字典中\n",
    "        # 然后把这个GO的所有is_a part of 关系的GO ID加入到children中\n",
    "        for term_id, val in ont.items():\n",
    "            if 'children' not in val:\n",
    "                val['children'] = set()\n",
    "            for p_id in val['is_a'] + val['part_of']:\n",
    "                if p_id in ont:\n",
    "                    if 'children' not in ont[p_id]:\n",
    "                        ont[p_id]['children'] = set()\n",
    "                    ont[p_id]['children'].add(term_id)\n",
    "        return ont, format_version, data_version\n",
    "\n",
    "    # ------------------------------------\n",
    "    def has_term(self, term_id):\n",
    "        return term_id in self.ont\n",
    "\n",
    "    def get_term(self, term_id):\n",
    "        if self.has_term(term_id):\n",
    "            return self.ont[term_id]\n",
    "        return None\n",
    "\n",
    "    def calculate_ic(self, annots):\n",
    "        cnt = Counter()\n",
    "        for x in annots:\n",
    "            cnt.update(x)\n",
    "        self.ic = {}\n",
    "        for go_id, n in cnt.items():\n",
    "            parents = self.get_parents(go_id)\n",
    "            if len(parents) == 0:\n",
    "                min_n = n\n",
    "            else:\n",
    "                min_n = min([cnt[x] for x in parents])\n",
    "\n",
    "            self.ic[go_id] = math.log(min_n / n, 2)\n",
    "\n",
    "    def get_ic(self, go_id):\n",
    "        if self.ic is None:\n",
    "            raise Exception('Not yet calculated')\n",
    "        if go_id not in self.ic:\n",
    "            return 0.0\n",
    "        return self.ic[go_id]\n",
    "\n",
    "    # revised 'part_of'\n",
    "    # 获得GO的所有祖先  这个获取的GO关系更加广泛 获得了GO的父元素，以及父元素的所有父元素 相当于把这个种族的家族都拿遍了\n",
    "    def get_ancestors(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        q = deque()  # 双端队列\n",
    "        q.append(term_id)  # 先将当前的GO加入队列中\n",
    "        # 当队列不为空时，从队列中弹出一个GO术语，如果这个GO不在term_set中，就加入，并把这个GO的所有父元素加入队列\n",
    "        while (len(q) > 0):\n",
    "            t_id = q.popleft()\n",
    "            if t_id not in term_set:\n",
    "                term_set.add(t_id)\n",
    "                for parent_id in (self.ont[t_id]['is_a'] +\n",
    "                                  self.ont[t_id]['part_of']):\n",
    "                    if parent_id in self.ont:\n",
    "                        q.append(parent_id)\n",
    "        # terms_set.remove(term_id)\n",
    "        return term_set\n",
    "\n",
    "    # revised\n",
    "    # 获得GO的父节点  这个获取是小范围的，只获得与指定GO关系为is_a part of 的GO\n",
    "    def get_parents(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        # 获得GO ID的所有父元素： 父元素 与这个GO关系为is_a part of的GO\n",
    "        for parent_id in (self.ont[term_id]['is_a'] +\n",
    "                          self.ont[term_id]['part_of']):\n",
    "            if parent_id in self.ont:\n",
    "                term_set.add(parent_id)\n",
    "        return term_set\n",
    "\n",
    "    # get the root terms(only is_a)\n",
    "    # 获得的是GO的根祖先 即GO的is_a关系以及父节点的所有is_a关系  就是祖先集合的只包含is_a关系的版本 近亲\n",
    "    def get_root_ancestors(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        q = deque()\n",
    "        q.append(term_id)\n",
    "        while (len(q) > 0):\n",
    "            t_id = q.popleft()\n",
    "            if t_id not in term_set:\n",
    "                term_set.add(t_id)\n",
    "                for parent_id in self.ont[t_id]['is_a']:\n",
    "                    if parent_id in self.ont:\n",
    "                        q.append(parent_id)\n",
    "        # terms_set.remove(term_id)\n",
    "        return term_set\n",
    "\n",
    "    # 获得GO的所有根元素\n",
    "    def get_roots(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        root_set = set()\n",
    "        for term in self.get_root_ancestors(term_id): # 遍历该GO的所有根祖先\n",
    "            if term not in self.ont:\n",
    "                continue\n",
    "            # 如果该GO的父元素为空 那么就是根元素\n",
    "            if len(self.get_parents(term)) == 0:\n",
    "                root_set.add(term)\n",
    "\n",
    "        return root_set\n",
    "\n",
    "    def get_namespace_terms(self, namespace):\n",
    "        terms = set()\n",
    "        for go_id, obj in self.ont.items():\n",
    "            if obj['namespace'] == namespace:\n",
    "                terms.add(go_id)\n",
    "        return terms\n",
    "\n",
    "    def get_namespace(self, term_id):\n",
    "        return self.ont[term_id]['namespace']\n",
    "\n",
    "    # all children\n",
    "    # 获得该GO的所有孩子元素 BFS搜索 从term_id开始逐层向下搜索所有的子术语 直到没有  这个获得了所有的孩子 即获得GO的子元素后，又纳入子元素的子元素\n",
    "    def get_term_set(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        q = deque()\n",
    "        q.append(term_id)\n",
    "        while len(q) > 0:\n",
    "            t_id = q.popleft()\n",
    "            if t_id not in term_set:\n",
    "                term_set.add(t_id)\n",
    "                for ch_id in self.ont[t_id]['children']:\n",
    "                    q.append(ch_id)\n",
    "        return term_set\n",
    "\n",
    "    # only one layer children\n",
    "    # 获得该GO的一层孩子元素  只获得一层孩子元素\n",
    "    def get_child_set(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        if term_id not in term_set:\n",
    "            for ch_id in self.ont[term_id]['children']:\n",
    "                term_set.add(ch_id)\n",
    "        return term_set\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# functions for evaluation\n",
    "def get_matrix(labels, preds, threshold=0.3):\n",
    "    preds = preds.flatten()\n",
    "    preds[preds >= threshold] = 1\n",
    "    preds = preds.astype('int8')\n",
    "    tn, fp, fn, tp = confusion_matrix(labels.flatten(), preds).ravel()\n",
    "    return tn, fp, fn, tp\n",
    "\n",
    "\n",
    "def get_level_matrix(labels, preds, level, threshold=0.3):\n",
    "    preds = preds[..., level]\n",
    "    preds = preds.flatten()\n",
    "    preds[preds >= threshold] = 1\n",
    "    preds = preds.astype('int8')\n",
    "    labels = labels[..., level]\n",
    "    tn, fp, fn, tp = confusion_matrix(labels.flatten(), preds).ravel()\n",
    "    return tn, fp, fn, tp\n",
    "\n",
    "\n",
    "def compute_roc(labels, preds):\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr, tpr, _ = roc_curve(labels.flatten(), preds.flatten())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "def compute_aupr_level(labels, preds, level):\n",
    "    labels = labels[..., level]\n",
    "    preds = preds[..., level]\n",
    "    precision, recall, _ = precision_recall_curve(labels.flatten(),\n",
    "                                                  preds.flatten())\n",
    "    aupr = auc(recall, precision)\n",
    "    return aupr\n",
    "\n",
    "\n",
    "def compute_aupr(labels, preds):\n",
    "    precision, recall, _ = precision_recall_curve(labels.flatten(),\n",
    "                                                  preds.flatten())\n",
    "    aupr = auc(recall, precision)\n",
    "    return aupr\n",
    "\n",
    "\n",
    "# set random seed\n",
    "def set_random_seed(seed=10, deterministic=False, benchmark=False):\n",
    "    # random.seed(seed)\n",
    "    # np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    if benchmark:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "# contrast training\n",
    "class con_pair_dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 con_pair,\n",
    "                 contrast_dict,\n",
    "                 terms,\n",
    "                 terms_dict,\n",
    "                 neg_num=80,\n",
    "                 neg=0.5,\n",
    "                 neg1_len=0.25):\n",
    "        super().__init__()\n",
    "        self.len_df = len(con_pair)\n",
    "        self.n_cc = list(contrast_dict['n_cc'])\n",
    "        self.n_bp = list(contrast_dict['n_bp'])\n",
    "        self.n_mf = list(contrast_dict['n_mf'])\n",
    "        self.terms = terms\n",
    "        self.contrast_dict = contrast_dict\n",
    "        self.terms_dict = terms_dict\n",
    "        self.neg_num = neg_num\n",
    "        self.con_pair = con_pair\n",
    "        self.neg = neg\n",
    "        self.neg1_len = neg1_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        terms_list = [self.con_pair[idx][0], self.con_pair[idx][1]]\n",
    "        negs1 = set()\n",
    "        neg1_len = min(len(self.con_pair[idx][2][0]),\n",
    "                       int(self.neg_num * self.neg1_len))\n",
    "        if neg1_len > 0:\n",
    "            negs1 = set(random.sample(self.con_pair[idx][2][0], k=neg1_len))\n",
    "        negs1 = list(negs1)\n",
    "        random.shuffle(negs1)\n",
    "\n",
    "        negs2 = set()\n",
    "        neg2_len = int((self.neg_num - neg1_len) * self.neg)\n",
    "        if len(self.contrast_dict[self.con_pair[idx][0]]) <= neg2_len:\n",
    "            negs2 = negs2 | set(\n",
    "                random.sample(self.contrast_dict[self.con_pair[idx][0]],\n",
    "                              k=len(\n",
    "                                  self.contrast_dict[self.con_pair[idx][0]])))\n",
    "            negs2 = negs2 | set(\n",
    "                random.sample(self.contrast_dict[self.con_pair[idx][0]],\n",
    "                              k=neg2_len -\n",
    "                              len(self.contrast_dict[self.con_pair[idx][0]])))\n",
    "        else:\n",
    "            negs2 = negs2 | set(\n",
    "                random.sample(self.contrast_dict[self.con_pair[idx][0]],\n",
    "                              k=neg2_len))\n",
    "        negs2 = list(negs2)\n",
    "        random.shuffle(negs2)\n",
    "\n",
    "        neg_len = neg1_len + neg2_len\n",
    "        neg_num = self.neg_num - neg_len\n",
    "        negs3 = set()\n",
    "        if self.contrast_dict[self.terms[terms_list[0]]] == 'GO:0005575':\n",
    "            while len(negs3) < neg_num // 3:\n",
    "                m = randint(0, len(self.n_mf) - 1)\n",
    "                if self.terms_dict[self.n_mf[m]] not in negs3:\n",
    "                    negs3.add(self.terms_dict[self.n_mf[m]])\n",
    "            while len(negs3) < neg_num:\n",
    "                m = randint(0, len(self.n_bp) - 1)\n",
    "                if self.terms_dict[self.n_bp[m]] not in negs3:\n",
    "                    negs3.add(self.terms_dict[self.n_bp[m]])\n",
    "        elif self.contrast_dict[self.terms[terms_list[0]]] == 'GO:0003674':\n",
    "            while len(negs3) < neg_num // 5:\n",
    "                m = randint(0, len(self.n_cc) - 1)\n",
    "                if self.terms_dict[self.n_cc[m]] not in negs3:\n",
    "                    negs3.add(self.terms_dict[self.n_cc[m]])\n",
    "            while len(negs3) < neg_num:\n",
    "                m = randint(0, len(self.n_bp) - 1)\n",
    "                if self.terms_dict[self.n_bp[m]] not in negs3:\n",
    "                    negs3.add(self.terms_dict[self.n_bp[m]])\n",
    "        elif self.contrast_dict[self.terms[terms_list[0]]] == 'GO:0008150':\n",
    "            while len(negs3) < neg_num // 3:\n",
    "                m = randint(0, len(self.n_cc) - 1)\n",
    "                if self.terms_dict[self.n_cc[m]] not in negs3:\n",
    "                    negs3.add(self.terms_dict[self.n_cc[m]])\n",
    "            while len(negs3) < neg_num:\n",
    "                m = randint(0, len(self.n_mf) - 1)\n",
    "                if self.terms_dict[self.n_mf[m]] not in negs3:\n",
    "                    negs3.add(self.terms_dict[self.n_mf[m]])\n",
    "        negs3 = list(negs3)\n",
    "        random.shuffle(negs3)\n",
    "\n",
    "        neg1_num = [neg1_len for i in range(neg1_len)]\n",
    "        neg2_num = [neg2_len for i in range(neg2_len)]\n",
    "        neg3_num = [neg_num for i in range(neg_num)]\n",
    "        neg_num = neg1_num + neg2_num + neg3_num\n",
    "        neg_num = 1 / np.array(neg_num)\n",
    "        terms_list = terms_list + negs1 + negs2 + negs3\n",
    "        return torch.LongTensor(terms_list).view(\n",
    "            len(terms_list)), torch.from_numpy(neg_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用GO类 计算处理GO官网下载得到的obo数据 用于获得每个GO的祖先 ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "from collections import defaultdict as ddt\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='extract all terms in a go.obo file.',\n",
    "                                 add_help=False)\n",
    "parser.add_argument('--go-file',\n",
    "                    '-gf',\n",
    "                    default=r'E:\\xj\\Paper\\PO2GO\\protein-annotation-master\\data\\go-basic.obo',\n",
    "                    type=str,\n",
    "                    help='go file downloaded from Gene Ontology website')\n",
    "parser.add_argument('--terms-file',\n",
    "                    '-tf',\n",
    "                    default=r'E:\\xj\\Paper\\PO2GO\\protein-annotation-master\\data\\terms_all.pkl',\n",
    "                    type=str,\n",
    "                    help='A DataFrame stored all terms')\n",
    "parser.add_argument('--out-ancestor',\n",
    "                    '-oa',\n",
    "                    default=r'E:\\xj\\Paper\\PO2GO\\protein-annotation-master\\data\\go_ancestor.pkl',\n",
    "                    type=str,\n",
    "                    help='output file for ancestor')\n",
    "parser.add_argument('--out-children',\n",
    "                    '-oc',\n",
    "                    default=r'E:\\xj\\Paper\\PO2GO\\protein-annotation-master\\data\\go_children.pkl',\n",
    "                    type=str,\n",
    "                    help='output file for children')\n",
    "\n",
    "\n",
    "\n",
    "def main(go_file, terms_all_file, out_ancestor, out_children):\n",
    "    # INPUT FILES\n",
    "    go = Ontology(go_file, with_rels=True, include_alt_ids=False)\n",
    "    # 读取所有GO ID\n",
    "    with open(terms_all_file, 'rb') as fd:\n",
    "        terms = pickle.load(fd)\n",
    "        terms = list(terms['terms'])\n",
    "    terms_set = set(terms)  # GO ID 集合\n",
    "    terms_dict = {v: i for i, v in enumerate(terms)}  # 构建GO映射 如{'GO:0000001': 0, 'GO:0000002': 1, ...}\n",
    "    # one layer parents, no self\n",
    "    # parents_dict 获得并存储每个GO术语的父元素—— is_a part of\n",
    "    parents_dict = ddt(set)  # 创建默认空字典 如果访问key不存在时不报错，返回默认值\n",
    "    for i in range(len(terms)):\n",
    "        parents_dict[terms[i]] = terms_set.intersection(go.get_parents(terms[i]))\n",
    "    \n",
    "    # all ancestors, no self\n",
    "    # ancestor_dict 获得每个GO的所有祖先元素 这个获取比父元素更为广泛 获得的是自己的父元素，以及所有父元素的父元素\n",
    "    ancestor_dict = ddt(set)\n",
    "    for i in range(len(terms)):\n",
    "        temp_set = go.get_ancestors(terms[i])\n",
    "        temp_set.remove(terms[i])\n",
    "        ancestor_dict[terms[i]] = terms_set.intersection(temp_set)\n",
    "    \n",
    "    # 获得所有GO各自的根元素 一个GO有相关的父元素，通过BFS找到这个GO家族的根元素 可能有多个\n",
    "    root_dict = ddt(set)\n",
    "    for i in range(len(terms)):\n",
    "        root_dict[terms[i]] = go.get_roots(terms[i])\n",
    "    # 这里简化了字典 只保留了一个根元素 如{'GO:0000001': {1,2,3,4,5}} 变为{'GO:0000001': 1}\n",
    "    for k, v in root_dict.items():\n",
    "        root_dict[k] = list(v)[0]\n",
    "    # 获得所有GO各自的所有子元素 即获得该GO的子元素后，继续获得子元素的子元素，直到没有子元素\n",
    "    child_dict = ddt(set)\n",
    "    for i in range(len(terms)):\n",
    "        child_dict[terms[i]] = terms_set.intersection(go.get_term_set(terms[i]))\n",
    "    # 获得所有GO各自的所有子元素 这个只有一层子元素 只获得该GO的子元素 不往后获得孙子辈的\n",
    "    child_one_dict = ddt(set)\n",
    "    for i in range(len(terms)):\n",
    "        child_one_dict[terms[i]] = terms_set.intersection(\n",
    "            go.get_child_set(terms[i]))\n",
    "\n",
    "    go_ancestor = ddt(list)\n",
    "    go_children = ddt(list)\n",
    "    count = 0\n",
    "    for i in terms:  # 遍历所有 GO ID\n",
    "        temp_anc_set = ancestor_dict[i]  # 获得该GO的所有祖先元素 即该GO的父元素，以及所有父元素的父元素\n",
    "        temp_child_set = go.get_term_set(i)  # 获得该GO的所有子元素 即该GO的子元素，以及所有子元素的子元素\n",
    "\n",
    "        go_ancestor[i] = list(temp_anc_set)\n",
    "        go_children[i] = list(temp_child_set)\n",
    "        print('{} is ok'.format(count))\n",
    "        count += 1\n",
    "\n",
    "\n",
    "    with open(r'E:\\xj\\Paper\\PO2GO\\protein-annotation-master\\data\\go_ancestor.pkl', 'wb') as fd:\n",
    "        pickle.dump(go_ancestor, fd)\n",
    "\n",
    "    with open(r'E:\\xj\\Paper\\PO2GO\\protein-annotation-master\\data\\go_children.pkl', 'wb') as fd:\n",
    "        pickle.dump(go_children, fd)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # args = parser.parse_args()\n",
    "    # main(args.go_file, args.terms_file, args.out_ancestor, args.out_children)\n",
    "\n",
    "    import pickle\n",
    "\n",
    "    with open(r'E:\\xj\\Paper\\PO2GO\\protein-annotation-master\\data\\go_ancestor.pkl', 'rb') as f:\n",
    "        ancestor_data = pickle.load(f)\n",
    "    print(len(ancestor_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取pkl文件 查看\n",
    "\n",
    "import pickle\n",
    "\n",
    "# 读取pkl文件\n",
    "with open(r'E:\\xj\\MyPaper\\MyPaper\\paper1\\GO\\go_ancestor.pkl', 'rb') as f:\n",
    "    go_ancestors = pickle.load(f)\n",
    "print(go_ancestors['GO:0000001'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "蛋白质的GO信息获取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先通过序列BLAST比对获得uniprot id\n",
    "方法： 下载uniprot数据库 使用本地BLAST对比 获得blast输出结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blast输出结果转csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def read_blast_results(blast_file):\n",
    "    with open(blast_file, 'r') as f:\n",
    "        # 只保留前三列\n",
    "        return [line.strip().split('\\t')[:3] for line in f]\n",
    "\n",
    "def write_blast_results_to_csv(blast_results, csv_file):\n",
    "    with open(csv_file, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # 写入列名\n",
    "        writer.writerow(['SampleID', 'MatchID', 'Similarity'])\n",
    "        # 写入数据\n",
    "        writer.writerows(blast_results)\n",
    "\n",
    "# 读取BLAST结果\n",
    "blast_results = read_blast_results(r'D:\\MyPaper\\paper1\\data\\eSOL\\eSOL_go\\eSOL_test_100.blast')\n",
    "\n",
    "# 将BLAST结果写入CSV文件\n",
    "write_blast_results_to_csv(blast_results, r'D:\\MyPaper\\paper1\\data\\eSOL\\eSOL_go\\eSOL_test_blast_100.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对csv文件进行去重和相似度划分 找到uniprot比对到的蛋白质和没有比对到的蛋白质"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(r'D:\\MyPaper\\paper1\\data\\eSOL\\eSOL_go\\eSOL_test_blast_100.csv')\n",
    "\n",
    "# 对样本ID进行排序，然后按照样本ID进行分组\n",
    "groups = df.sort_values('Similarity', ascending=False).groupby('SampleID')\n",
    "\n",
    "# 对每个组，只保留相似度最高的行\n",
    "df_unique = groups.first().reset_index()\n",
    "\n",
    "# 将结果写入新的CSV文件\n",
    "df_unique.to_csv(r'D:\\MyPaper\\paper1\\data\\eSOL\\eSOL_go\\eSOL_test_blast_100.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理csv文件，获得uniprot.txt文件，用于脚本自动化搜索获得GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "df_100 = pd.read_csv(r'D:\\MyPaper\\paper1\\data\\eSOL\\eSOL_go\\eSOL_test_blast_100.csv')\n",
    "\n",
    "# # 选择Similarity列等于100的行\n",
    "# df_100 = df[df['Similarity'] == 100]\n",
    "\n",
    "# 从MatchID列切出Uniprot ID\n",
    "df_100['UniprotID'] = df_100['MatchID'].apply(lambda x: x.split('|')[1])\n",
    "\n",
    "# 将Uniprot ID写入TXT文件\n",
    "df_100['UniprotID'].to_csv(r'D:\\MyPaper\\paper1\\data\\eSOL\\eSOL_go\\eSOL_test_blast_100_uniprot_ids.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "脚本自动化搜索获得GO\n",
    "可以直接在uniprot上批量查询下载txt文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理text文件 获得对应的GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_go_annotations_from_text(text):\n",
    "    go_annotations = []\n",
    "    lines = text.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        if line.startswith(\"DR   GO\"):\n",
    "            # Extracting GO annotations from the DR line\n",
    "            go_info = line.split(\";\")[1].strip()\n",
    "            go_annotations.append(go_info)\n",
    "    return go_annotations\n",
    "\n",
    "def main(input_file, output_file):\n",
    "    with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "        text = infile.read()\n",
    "        # 使用正则表达式分割文本，获取每个Uniprot ID的信息\n",
    "        uniprot_infos = re.split(\"//\\n\", text)\n",
    "        for uniprot_info in uniprot_infos:\n",
    "            # 获取Uniprot ID\n",
    "            match = re.search(\"AC   (\\w+);\", uniprot_info)  # 修改这里，从\"AC\"行获取Uniprot ID\n",
    "            if match:\n",
    "                uniprot_id = match.group(1)\n",
    "                # 获取GO注释\n",
    "                go_annotations = get_go_annotations_from_text(uniprot_info)\n",
    "                outfile.write(f\"{uniprot_id}: {', '.join(go_annotations)}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file_path = r\"D:\\MyPaper\\paper1\\data\\eSOL\\eSOL_go\\eSOL_test_blast_100_uniprot_search_result.txt\"  # 替换为实际的输入文件路径\n",
    "    output_file_path = r\"D:\\MyPaper\\paper1\\data\\eSOL\\eSOL_go\\eSOL_test_100_uniprot_go_conform.txt\"  # 替换为实际的输出文件路径\n",
    "\n",
    "    main(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后UniProt ID映射回原样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假设原始数据文件和映射文件的路径\n",
    "original_txt_path = r\"D:\\MyPaper\\paper1\\data\\eSOL\\eSOL_go\\eSOL_test_100_uniprot_go_conform.txt\"\n",
    "mapping_csv_path = r\"D:\\MyPaper\\paper1\\data\\eSOL\\eSOL_go\\eSOL_test_blast_100.csv\"\n",
    "updated_txt_path = r\"D:\\MyPaper\\paper1\\data\\eSOL\\eSOL_go\\eSOL_test_protein2go.txt\"\n",
    "\n",
    "# 读取映射文件\n",
    "id_mapping_df = pd.read_csv(mapping_csv_path)\n",
    "\n",
    "# 创建从UniProt ID到新样本ID的映射字典\n",
    "id_mapping = {}\n",
    "for _, row in id_mapping_df.iterrows():\n",
    "    uni_prot_id = row['MatchID'].split('|')[1]  # 提取UniProt ID\n",
    "    if uni_prot_id not in id_mapping:\n",
    "        id_mapping[uni_prot_id] = []\n",
    "    id_mapping[uni_prot_id].append(row['SampleID'])\n",
    "\n",
    "# 读取原始txt文件\n",
    "with open(original_txt_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# 更新样本ID\n",
    "updated_lines = []\n",
    "for line in lines:\n",
    "    if ': GO' in line:\n",
    "        parts = line.strip().split(': ', 1)  # 分割UniProt ID和GO信息\n",
    "    else:\n",
    "        parts = line.strip().split(':', 1)  # 分割UniProt ID和GO信息\n",
    "    uni_prot_id = parts[0]\n",
    "    go_info = parts[1] if len(parts) > 1 else ''\n",
    "    # if uni_prot_id in id_mapping:\n",
    "    #     # 对于每个匹配的UniProt ID，为每个对应的样本ID创建一行\n",
    "    #     for sample_id in id_mapping[uni_prot_id]:\n",
    "    #         updated_line = f\"{sample_id}: {go_info}\\n\"  # 保持原始格式不变\n",
    "    #         updated_lines.append(updated_line)\n",
    "    # else:\n",
    "    #     # 如果没有映射，保留原行\n",
    "    #     updated_lines.append(line)\n",
    "    # 对于每个匹配的UniProt ID，为每个对应的样本ID创建一行\n",
    "    for sample_id in id_mapping[uni_prot_id]:\n",
    "        updated_line = f\"{sample_id}: {go_info}\\n\"  # 保持原始格式不变\n",
    "        updated_lines.append(updated_line)\n",
    "\n",
    "# 将更新后的数据写入新的txt文件\n",
    "with open(updated_txt_path, 'w') as file:\n",
    "    file.writelines(updated_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到GO数据库的所有GO各自祖先信息，以及蛋白质对应的GO ID\n",
    "下一步将每个蛋白质的GO归纳到UniProt的GO描述小类中 这里划分了BP MF CC三类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 读取pkl文件\n",
    "with open(r'D:\\MyPaper\\paper1\\data\\go_ancestor.pkl', 'rb') as f:\n",
    "    go_ancestors = pickle.load(f)\n",
    "\n",
    "# 读取txt文件\n",
    "with open(r'D:\\MyPaper\\paper1\\data\\eSOL\\eSOL_go\\eSOL_test_protein2go.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 以下GO ID 来自UniProtKB GOA的GO ID\n",
    "mf_keys = ['GO:0001618', 'GO:0003677', 'GO:0003723', 'GO:0003774', 'GO:0003824', 'GO:0003924', 'GO:0005198', 'GO:0005215', 'GO:0008092', \n",
    "           'GO:0008289', 'GO:0009975', 'GO:0016209', 'GO:0016491', 'GO:0016740', 'GO:0016787', 'GO:0016829', 'GO:0016853', 'GO:0016874', \n",
    "           'GO:0031386', 'GO:0038024', 'GO:0042393', 'GO:0044183', 'GO:0045182', 'GO:0045735', 'GO:0048018', 'GO:0060089', 'GO:0060090',\n",
    "           'GO:0090729', 'GO:0098631', 'GO:0098772', 'GO:0120274', 'GO:0140096', 'GO:0140097', 'GO:0140098', 'GO:0140104', 'GO:0140110',\n",
    "           'GO:0140223', 'GO:0140299', 'GO:0140313', 'GO:0140657', 'GO:0003674']\n",
    "\n",
    "bp_keys = ['GO:0000278', 'GO:0000910', 'GO:0002181', 'GO:0002376', 'GO:0003012', 'GO:0003013', 'GO:0003014', 'GO:0003016', 'GO:0005975',\n",
    "           'GO:0006091', 'GO:0006260', 'GO:0006281', 'GO:0006310', 'GO:0006325', 'GO:0006351', 'GO:0006355', 'GO:0006399', 'GO:0006457',\n",
    "           'GO:0006486', 'GO:0006520', 'GO:0006575', 'GO:0006629', 'GO:0006766', 'GO:0006790', 'GO:0006886', 'GO:0006913', 'GO:0006914',\n",
    "           'GO:0006954', 'GO:0007005', 'GO:0007010', 'GO:0007018', 'GO:0007031', 'GO:0007040', 'GO:0007059', 'GO:0007155', 'GO:0007163',\n",
    "           'GO:0012501', 'GO:0015979', 'GO:0016071', 'GO:0016073', 'GO:0016192', 'GO:0022414', 'GO:0022600', 'GO:0023052', 'GO:0030154',\n",
    "           'GO:0030163', 'GO:0030198', 'GO:0031047', 'GO:0032200', 'GO:0034330', 'GO:0042060', 'GO:0044782', 'GO:0048856', 'GO:0048870',\n",
    "           'GO:0050877', 'GO:0050886', 'GO:0051604', 'GO:0055085', 'GO:0055086', 'GO:0061007', 'GO:0061024', 'GO:0065003', 'GO:0071554',\n",
    "           'GO:0071941', 'GO:0072659', 'GO:0098542', 'GO:0098754', 'GO:0140013', 'GO:0140014', 'GO:0140053', 'GO:1901135', 'GO:1901135',\n",
    "           'GO:0008150']\n",
    "\n",
    "cc_keys = ['GO:0000228', 'GO:0005576', 'GO:0005615', 'GO:0005618', 'GO:0005634', 'GO:0005635', 'GO:0005654', 'GO:0005694', 'GO:0005730',\n",
    "           'GO:0005739', 'GO:0005764', 'GO:0005768', 'GO:0005773', 'GO:0005777', 'GO:0005783', 'GO:0005794', 'GO:0005811', 'GO:0005815',\n",
    "           'GO:0005829', 'GO:0005840', 'GO:0005856', 'GO:0005886', 'GO:0005929', 'GO:0009536', 'GO:0009579', 'GO:0030312', 'GO:0031012',\n",
    "           'GO:0031410', 'GO:0043226', 'GO:0005575']\n",
    "\n",
    "protein_go = {}  # 创建一个字典 用于存储蛋白质的GO信息\n",
    "\n",
    "protein_conform_go = {}  \n",
    "\n",
    "# 创建一个函数来根据One-hot向量的索引找到对应的GO id\n",
    "def get_go_ids(one_hot_vector, go_ids):\n",
    "    return [go_ids[i] for i, val in enumerate(one_hot_vector) if val == 1]\n",
    "\n",
    "\n",
    "# 大循环遍历每一行 每一行包含蛋白质的ID 以及对应的GO list\n",
    "for line in lines:\n",
    "    mf_mutil_vec = [0]*len(mf_keys)  # 创建该蛋白质的MF MUTI-HOT VECTOR\n",
    "    bp_mutil_vec = [0]*len(bp_keys)  # 创建该蛋白质的BP MUTI-HOT VECTOR\n",
    "    cc_mutil_vec = [0]*len(cc_keys)  # 创建该蛋白质的CC MUTI-HOT VECTOR\n",
    "    mf_one_vec = [0]*len(mf_keys)  # 创建该蛋白质的MF ONE-HOT VECTOR\n",
    "    bp_one_vec = [0]*len(bp_keys)  # 创建该蛋白质的BP ONE-HOT VECTOR\n",
    "    cc_one_vec = [0]*len(cc_keys)  # 创建该蛋白质的CC ONE-HOT VECTOR\n",
    "    \n",
    "    # 分割每一行为UniPort ID和GO信息\n",
    "    parts = line.strip().split(': ')\n",
    "    # 如果len(parts) < 2 说明这个蛋白质没有GO 构建成全0的向量即可\n",
    "    if len(parts) < 2:\n",
    "                # print(f\"Invalid line: {parts[0]}\")\n",
    "                id = parts[0][:-1]\n",
    "                protein_go[id] = {'MF': {'mutil': mf_mutil_vec, 'one': mf_one_vec},\n",
    "                              'BP': {'mutil': bp_mutil_vec, 'one': bp_one_vec},\n",
    "                              'CC': {'mutil': cc_mutil_vec, 'one': cc_one_vec}}\n",
    "                continue\n",
    "    uniprot_id = parts[0]\n",
    "    go_info = parts[1]\n",
    "    # 将GO信息分割为多个GO\n",
    "    go_list = go_info.split(', ')  # 获得该蛋白质的GO LIST\n",
    "    \n",
    "\n",
    "    # 循环遍历该蛋白质的每个GO ID  填充到三个对应的vec中\n",
    "    for go in go_list:\n",
    "        # 如果该蛋白质的这个GO id 在GO数据库中 即在go_ancestors中 \n",
    "        if go in go_ancestors.keys():\n",
    "            # MF 类填充\n",
    "            for index, mf_key in enumerate(mf_keys):  # 遍历MF的所有小类的GO id\n",
    "                if mf_key in  go_ancestors[go]:  # 如果当前遍历的MF小类的GO id 在该蛋白质的这个GO的祖先列表中 说明该GO属于这个小类 is_a / part of回溯到了这个叶子节点\n",
    "                    mf_mutil_vec[index] += 1  # 所以在mutil_vec中，当前索引位置+1\n",
    "                    mf_one_vec[index] = 1  # 在one_vec中，当前索引位置修改为1\n",
    "            # BP 类填充\n",
    "            for index, bp_key in enumerate(bp_keys):\n",
    "                if bp_key in go_ancestors[go]:\n",
    "                    bp_mutil_vec[index] += 1\n",
    "                    bp_one_vec[index] = 1\n",
    "            # CC 类填充\n",
    "            for index, cc_key in enumerate(cc_keys):\n",
    "                if cc_key in go_ancestors[go]:\n",
    "                    cc_mutil_vec[index] += 1\n",
    "                    cc_one_vec[index] = 1\n",
    "        else:\n",
    "            print(f\"GO not found: {go}\")\n",
    "    \n",
    "    # 现在获得了这个蛋白质的MF BP CC的MUTIL-HOT VECTOR 和 ONE-HOT VECTOR 下一步要把这三类存储到字典中\n",
    "    protein_go[uniprot_id] = {'MF': {'mutil': mf_mutil_vec, 'one': mf_one_vec},\n",
    "                              'BP': {'mutil': bp_mutil_vec, 'one': bp_one_vec},\n",
    "                              'CC': {'mutil': cc_mutil_vec, 'one': cc_one_vec}}\n",
    "    # 存储有对应 GO ID 的项\n",
    "    go_mf_ids = [mf_keys[i] for i in range(len(mf_keys)) if mf_one_vec[i] == 1]\n",
    "    go_bp_ids = [bp_keys[i] for i in range(len(bp_keys)) if bp_one_vec[i] == 1]\n",
    "    go_cc_ids = [cc_keys[i] for i in range(len(cc_keys)) if cc_one_vec[i] == 1]\n",
    "    go_total = go_mf_ids + go_bp_ids + go_cc_ids\n",
    "    protein_conform_go[uniprot_id] = go_total\n",
    "\n",
    "with open(r'D:\\MyPaper\\paper1\\data\\eSOL\\eSOL_go\\eSOL_test_100_uniprot_conform_protein2go.txt', 'w') as f:\n",
    "    for uniprot_id, go_total in protein_conform_go.items():\n",
    "        f.write(f\"{uniprot_id}: {', '.join(go_total)}\\n\")\n",
    "\n",
    "\n",
    "# 将protein_go字典存储到pkl文件中\n",
    "with open(r'D:\\MyPaper\\paper1\\data\\eSOL\\eSOL_go\\eSOL_test_100_uniprot_go_conform.pkl', 'wb') as f:\n",
    "    pickle.dump(protein_go, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取pkl文件检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: acpS, Value: 144\n"
     ]
    }
   ],
   "source": [
    "with open(r'D:\\MyPaper\\paper1\\data\\eSOL\\eSOL_go\\eSOL_test_uniprot_go_conform_mutil_concat.pkl', 'rb') as f:\n",
    "    protein_go = pickle.load(f)\n",
    "for key, value in protein_go.items():\n",
    "    print(f\"Key: {key}, Value: {len(value)}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拼接成一个GO向量 存成和原先GO_get中的pkl一致的版本 mutil和one分别存两版\n",
    "如果想要整合直接训 就直接读取直接训\n",
    "如果想要三类分开学 就用切片切开 \n",
    "拼接统一按MF+BP+CC 三类都是定长的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open(r'D:\\MyPaper\\paper1\\data\\eSOL\\eSOL_go\\eSOL_train_100_uniprot_go_conform.pkl', 'rb') as f:\n",
    "    protein_go = pickle.load(f)\n",
    "protein_go_mutil = {}\n",
    "protein_go_one = {}\n",
    "for key, value in protein_go.items():\n",
    "    protein_go_mutil[key] = []\n",
    "    protein_go_one[key] = []\n",
    "    # 按照MF BP CC三类进行拼接\n",
    "    for go_type in ['MF', 'BP', 'CC']:\n",
    "        protein_go_mutil[key].extend(value[go_type]['mutil'])  # 将当前蛋白质的MF BP CC的MUTIL-HOT VECTOR拼接到一起\n",
    "        protein_go_one[key].extend(value[go_type]['one'])  # 将当前蛋白质的MF BP CC的ONE-HOT VECTOR拼接到一起\n",
    "    # 转换为 NumPy 数组\n",
    "    protein_go_mutil[key] = np.array(protein_go_mutil[key])\n",
    "    protein_go_one[key] = np.array(protein_go_one[key])\n",
    "\n",
    "# 将拼接后的MUTIL-HOT VECTOR 和 ONE-HOT VECTOR 存储到pkl文件中\n",
    "with open(r'D:\\MyPaper\\paper1\\data\\eSOL\\eSOL_go\\eSOL_train_uniprot_go_conform_mutil_concat.pkl', 'wb') as f:\n",
    "    pickle.dump(protein_go_mutil, f)\n",
    "\n",
    "with open(r'D:\\MyPaper\\paper1\\data\\eSOL\\eSOL_go\\eSOL_train_uniprot_go_conform_onehot_concat.pkl', 'wb') as f:\n",
    "    pickle.dump(protein_go_one, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接上GO_get的后续ID替换拼接部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取pkl文件查看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: P07550, Value: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 8, 12]\n",
      "Key: P07550, Value: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 8, 12]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with open(r'E:\\xj\\MyPaper\\MyPaper\\paper1\\data\\temp\\uniprot_go_mutil_concat_E.coli_3100_1_900_rowid.pkl', 'rb') as f:\n",
    "    protein_go_rowid = pickle.load(f)\n",
    "for key, value in protein_go.items():\n",
    "    print(f\"Key: {key}, Value: {value}\")\n",
    "    break\n",
    "\n",
    "with open(r'E:\\xj\\MyPaper\\MyPaper\\paper1\\data\\temp\\uniprot_go_mutil_concat_E.coli_3100_1_900.pkl', 'rb') as f:\n",
    "    protein_go = pickle.load(f)\n",
    "for key, value in protein_go.items():\n",
    "    print(f\"Key: {key}, Value: {value}\")\n",
    "    break\n",
    "\n",
    "print(protein_go_rowid['BSGC-BSGCAIR30353'] == protein_go['Q50336'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将pickle文件的id对应到原数据的id 便于下一步和fasta文件对应，concat fea vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "train_df = pd.read_csv(r\"D:\\MyPaper\\paper1\\MutSol\\train_mut_blast_1.csv\")\n",
    "# test_df = pd.read_csv(r\"E:\\xj\\MyPaper\\MyPaper\\paper1\\data\\E.coli\\nesg_3100\\test_nesg_new_blast_1.csv\")\n",
    "# 创建从Uniprot ID到SampleID的映射\n",
    "# 创建从Uniprot ID到SampleID的映射\n",
    "train_id_mapping = {}\n",
    "# test_id_mapping = {}\n",
    "\n",
    "# 读取pkl文件\n",
    "with open(r'E:\\xj\\MyPaper\\MyPaper\\paper1\\data\\temp\\uniprot_go_one_concat_E.coli_3100_1_900.pkl', 'rb') as f:\n",
    "    train_vectors = pickle.load(f)\n",
    "# with open(r'E:\\xj\\MyPaper\\MyPaper\\paper1\\data\\temp\\uniprot_go_one_concat_test_nesg.pkl', 'rb') as f:\n",
    "#     test_vectors = pickle.load(f)\n",
    "for _, row in train_df.iterrows():\n",
    "    id = row['MatchID'].split('|')[1]\n",
    "    if id not in train_id_mapping:\n",
    "        train_id_mapping[id] = []\n",
    "    train_id_mapping[id].append(row['SampleID'])\n",
    "\n",
    "# for _, row in test_df.iterrows():\n",
    "#     id = row['MatchID'].split('|')[1]\n",
    "#     if id not in test_id_mapping:\n",
    "#         test_id_mapping[id] = []\n",
    "#     test_id_mapping[id].append(row['SampleID'])\n",
    "\n",
    "# 更新训练向量的ID\n",
    "train_vectors_updated = {}\n",
    "for id, vector in train_vectors.items():\n",
    "    if id in train_id_mapping:\n",
    "        sample_ids = train_id_mapping[id]\n",
    "        for sample_id in sample_ids:\n",
    "            train_vectors_updated[sample_id] = vector\n",
    "\n",
    "# # 更新测试向量的ID\n",
    "# test_vectors_updated = {}\n",
    "# for id, vector in test_vectors.items():\n",
    "#     if id in test_id_mapping:\n",
    "#         sample_ids = test_id_mapping[id]\n",
    "#         for sample_id in sample_ids:\n",
    "#             test_vectors_updated[sample_id] = vector\n",
    "\n",
    "# 保存更新后的训练向量\n",
    "with open(r\"E:\\xj\\MyPaper\\MyPaper\\paper1\\data\\temp\\uniprot_go_one_concat_E.coli_3100_1_900_rowid.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_vectors_updated, f)\n",
    "\n",
    "# with open(r\"E:\\xj\\MyPaper\\MyPaper\\paper1\\data\\temp\\uniprot_go_one_concat_test_nesg_rowid.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(test_vectors_updated, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证 ID 映射是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(r'E:\\xj\\MyPaper\\MyPaper\\paper1\\data\\temp\\uniprot_go_mutil_concat_eSOL_test_rowid.pkl', 'rb') as f:\n",
    "    protein_go_rowid = pickle.load(f)\n",
    "\n",
    "with open(r'E:\\xj\\MyPaper\\MyPaper\\paper1\\data\\temp\\uniprot_go_mutil_concat_eSOL_test.pkl', 'rb') as f:\n",
    "    protein_go = pickle.load(f)\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(r\"E:\\xj\\MyPaper\\MyPaper\\paper1\\data\\temp\\eSOL_test.csv\")\n",
    "# 创建从Uniprot ID到SampleID的映射\n",
    "id_mapping = {}\n",
    "for _, row in df.iterrows():\n",
    "    id = row['MatchID'].split('|')[1]\n",
    "    if id not in id_mapping:\n",
    "        id_mapping[id] = []\n",
    "    id_mapping[id].append(row['SampleID'])\n",
    "\n",
    "for key, value in protein_go.items():\n",
    "    if protein_go[key] != protein_go_rowid[id_mapping[key][0]]:\n",
    "        print(f\"Key: {key}, Value: {value}, Protein GO: {protein_go[key]}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把GO存到fasta后面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3100"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "import pickle\n",
    "\n",
    "# 读取pkl文件\n",
    "with open(r\"E:\\xj\\MyPaper\\MyPaper\\paper1\\data\\temp\\uniprot_go_one_concat_test_nesg_rowid.pkl\", \"rb\") as f:\n",
    "    train_vectors = pickle.load(f)\n",
    "\n",
    "# 读取fasta文件\n",
    "fasta_sequences = list(SeqIO.parse(r\"E:\\xj\\MyPaper\\MyPaper\\paper1\\data\\E.coli\\nesg_3100\\test_set_nesg_new_fea.fasta\", \"fasta\"))\n",
    "\n",
    "# 更新fasta文件中的feature\n",
    "for seq in fasta_sequences:\n",
    "    id = seq.id.split(' ')[0]  # 获取id\n",
    "    if id in train_vectors:\n",
    "        vector = train_vectors[id]  # 获取对应的向量\n",
    "        go_str = 'GO=[' + ', '.join(map(str, vector)) + ']'  # 创建新的GO字符串\n",
    "        seq.description = seq.description + ' ' + go_str  # 更新description\n",
    "    else:\n",
    "        go_str = 'GO=[' + ', '.join(['0']*144) + ']'  # 创建长度为144的全零向量\n",
    "        seq.description = seq.description + ' ' + go_str  # 更新description\n",
    "\n",
    "# 保存更新后的fasta文件\n",
    "SeqIO.write(fasta_sequences, r\"E:\\xj\\MyPaper\\MyPaper\\paper1\\data\\E.coli\\nesg_3100\\test_set_nesg_new_fea_GO_one_concat.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证是否匹配正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample ydfP not found in pkl file\n",
      "Sample yfjO not found in pkl file\n",
      "Sample ykiB not found in pkl file\n",
      "Sample ymfH not found in pkl file\n",
      "Sample ymfO not found in pkl file\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# 读取fasta文件\n",
    "fasta_sequences = list(SeqIO.parse(r\"E:\\xj\\MyPaper\\MyPaper\\paper1\\data\\eSOL\\eSOL_train_fea_GO_one_concat.fasta\", \"fasta\"))\n",
    "\n",
    "with open(r\"E:\\xj\\MyPaper\\MyPaper\\paper1\\data\\temp\\uniprot_go_one_concat_eSOL_train_rowid.pkl\", \"rb\") as f:\n",
    "    train_vectors = pickle.load(f)\n",
    "\n",
    "# 使用正则表达式从description中提取label和label_noise\n",
    "pattern = re.compile(r'label=(\\d+) label_noise=(\\d+) feature=\\[([^\\]]+)\\] GO=\\[([^\\]]+)\\]')\n",
    "\n",
    "# 用于存储提取的信息的列表\n",
    "fasta_go_list=[]        \n",
    "\n",
    "for seq in fasta_sequences:\n",
    "    match = pattern.search(seq.description)\n",
    "    if match:\n",
    "        id = seq.id.split(' ')[0]  # 获取id\n",
    "        fasta_go_list = [int(x) for x in match.group(4).split(',')]  # 获取GO列表\n",
    "\n",
    "        # 如果id在pkl文件中\n",
    "        if id in train_vectors:\n",
    "            pkl_go_list = train_vectors[id]  # 获取pkl文件中的GO列表\n",
    "\n",
    "            # 如果两个GO列表不一致，打印出来\n",
    "            if set(fasta_go_list) != set(pkl_go_list):\n",
    "                print(f\"Sample {id} GO list not match:\")\n",
    "                print(f\"Fasta GO list: {fasta_go_list}\")\n",
    "                print(f\"Pkl GO list: {pkl_go_list}\")\n",
    "            else:\n",
    "                # print(f\"Sample {id} GO list match\")\n",
    "                pass\n",
    "        else:\n",
    "            print(f\"Sample {id} not found in pkl file\")\n",
    "    else:\n",
    "        print(f\"Invalid description: {seq.description}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PENCIL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
